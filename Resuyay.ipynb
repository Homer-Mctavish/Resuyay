{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "960e9a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /home/absconditus/anaconda3/lib/python3.9/site-packages (3.4.3)\n",
      "Requirement already satisfied: numpy>=1.16 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from matplotlib) (1.20.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: six in /home/absconditus/anaconda3/lib/python3.9/site-packages (from cycler>=0.10->matplotlib) (1.12.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: numpy in /home/absconditus/anaconda3/lib/python3.9/site-packages (1.20.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pandas in /home/absconditus/anaconda3/lib/python3.9/site-packages (1.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from pandas) (1.20.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas) (1.12.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: tensorflow in /home/absconditus/anaconda3/lib/python3.9/site-packages (2.9.1)\n",
      "Requirement already satisfied: packaging in /home/absconditus/anaconda3/lib/python3.9/site-packages (from tensorflow) (21.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.10.0.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.47.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.19.4)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.9.1)\n",
      "Requirement already satisfied: setuptools in /home/absconditus/anaconda3/lib/python3.9/site-packages (from tensorflow) (63.2.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.20.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.26.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.10.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from packaging->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.8.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: zipp>=0.5 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: seaborn in /home/absconditus/anaconda3/lib/python3.9/site-packages (0.11.2)\n",
      "Requirement already satisfied: scipy>=1.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from seaborn) (1.7.1)\n",
      "Requirement already satisfied: pandas>=0.23 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from seaborn) (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from seaborn) (1.20.3)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from seaborn) (3.4.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (8.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from pandas>=0.23->seaborn) (2021.3)\n",
      "Requirement already satisfied: six in /home/absconditus/anaconda3/lib/python3.9/site-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.12.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: textract in /home/absconditus/anaconda3/lib/python3.9/site-packages (1.6.5)\n",
      "Requirement already satisfied: extract-msg<=0.29.* in /home/absconditus/anaconda3/lib/python3.9/site-packages (from textract) (0.28.7)\n",
      "Requirement already satisfied: SpeechRecognition~=3.8.1 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from textract) (3.8.1)\n",
      "Requirement already satisfied: beautifulsoup4~=4.8.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from textract) (4.8.2)\n",
      "Requirement already satisfied: docx2txt~=0.8 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from textract) (0.8)\n",
      "Requirement already satisfied: python-pptx~=0.6.18 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from textract) (0.6.21)\n",
      "Requirement already satisfied: argcomplete~=1.10.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from textract) (1.10.3)\n",
      "Requirement already satisfied: xlrd~=1.2.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from textract) (1.2.0)\n",
      "Requirement already satisfied: chardet==3.* in /home/absconditus/anaconda3/lib/python3.9/site-packages (from textract) (3.0.4)\n",
      "Requirement already satisfied: pdfminer.six==20191110 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from textract) (20191110)\n",
      "Requirement already satisfied: six~=1.12.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from textract) (1.12.0)\n",
      "Requirement already satisfied: sortedcontainers in /home/absconditus/anaconda3/lib/python3.9/site-packages (from pdfminer.six==20191110->textract) (2.4.0)\n",
      "Requirement already satisfied: pycryptodome in /home/absconditus/anaconda3/lib/python3.9/site-packages (from pdfminer.six==20191110->textract) (3.15.0)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from beautifulsoup4~=4.8.0->textract) (2.2.1)\n",
      "Requirement already satisfied: olefile>=0.46 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from extract-msg<=0.29.*->textract) (0.46)\n",
      "Requirement already satisfied: imapclient==2.1.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from extract-msg<=0.29.*->textract) (2.1.0)\n",
      "Requirement already satisfied: tzlocal>=2.1 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from extract-msg<=0.29.*->textract) (4.2)\n",
      "Requirement already satisfied: compressed-rtf>=1.0.6 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from extract-msg<=0.29.*->textract) (1.0.6)\n",
      "Requirement already satisfied: ebcdic>=1.1.1 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from extract-msg<=0.29.*->textract) (1.1.1)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from python-pptx~=0.6.18->textract) (4.6.3)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from python-pptx~=0.6.18->textract) (8.4.0)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from python-pptx~=0.6.18->textract) (3.0.1)\n",
      "Requirement already satisfied: pytz-deprecation-shim in /home/absconditus/anaconda3/lib/python3.9/site-packages (from tzlocal>=2.1->extract-msg<=0.29.*->textract) (0.1.0.post0)\n",
      "Requirement already satisfied: tzdata in /home/absconditus/anaconda3/lib/python3.9/site-packages (from pytz-deprecation-shim->tzlocal>=2.1->extract-msg<=0.29.*->textract) (2022.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: PyPDF2 in /home/absconditus/anaconda3/lib/python3.9/site-packages (2.10.0)\n",
      "Requirement already satisfied: typing-extensions in /home/absconditus/anaconda3/lib/python3.9/site-packages (from PyPDF2) (3.10.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement en_cor_web_sm (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for en_cor_web_sm\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: spacy in /home/absconditus/anaconda3/lib/python3.9/site-packages (2.3.7)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from spacy) (0.7.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: setuptools in /home/absconditus/anaconda3/lib/python3.9/site-packages (from spacy) (63.2.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from spacy) (1.20.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from spacy) (2.26.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.7)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from spacy) (7.4.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from spacy) (0.9.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: jsonlines in /home/absconditus/anaconda3/lib/python3.9/site-packages (3.1.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/absconditus/anaconda3/lib/python3.9/site-packages (from jsonlines) (21.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install tensorflow\n",
    "!{sys.executable} -m pip install seaborn\n",
    "!{sys.executable} -m pip install textract\n",
    "!{sys.executable} -m pip install PyPDF2\n",
    "!{sys.executable} -m pip install en_cor_web_sm\n",
    "!{sys.executable} -m pip install spacy\n",
    "!{sys.executable} -m pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24633e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import seaborn as sns\n",
    "import textract\n",
    "import PyPDF2\n",
    "import en_core_web_sm\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy import displacy\n",
    "import jsonlines\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08112bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file):\n",
    "    '''Opens and reads in a PDF file from path'''\n",
    "    \n",
    "    fileReader = PyPDF2.PdfFileReader(open(file,'rb'))\n",
    "    page_count = fileReader.getNumPages()\n",
    "    text = [fileReader.getPage(i).extractText() for i in range(page_count)]\n",
    "    \n",
    "    return str(text).replace(\"\\\\n\", \"\")\n",
    "\n",
    "def extract_text_from_word(filepath):\n",
    "    '''Opens and reads in a .doc or .docx file from path'''\n",
    "    \n",
    "    txt = textract.process(filepath).decode('utf-8')\n",
    "    \n",
    "    return txt.replace('\\n', ' ').replace('\\t', ' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa2bbdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85fbd9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained English language model\n",
    "# nlp = nl_core_news_sm.load()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# File Extension. set as 'pdf' or as 'doc(x)'\n",
    "extension = 'pdf'\n",
    "diro='/home/absconditus/Documents/Github/Python Projects/jupyter/TF LSTM RNN for cover letter generation/Workfiles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e783a559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenized_texts_list(extension):\n",
    "    '''Create two lists, one with the names of the candidate and one with the tokenized \n",
    "       resume texts extracted from either a .pdf or .doc'''\n",
    "    resume_texts, resume_names = [], []\n",
    "    \n",
    "    # Loop over the contents of the directory containing the resumes, filtering by .pdf or .doc(x)\n",
    "    for resume in list(filter(lambda x: extension in x, os.listdir(diro + '/CV'))):\n",
    "        if extension == 'pdf':\n",
    "            # Read in every resume with pdf extension in the directory\n",
    "            resume_texts.append(nlp(extract_text_from_pdf(diro + '/CV/' + resume)))\n",
    "        elif 'doc' in extension:\n",
    "            # Read in every resume with .doc or .docx extension in the directory\n",
    "            resume_texts.append(nlp(extract_text_from_word(diro + '/CV/' + resume)))\n",
    "            \n",
    "        resume_names.append(resume.split('_')[0].capitalize())\n",
    "        \n",
    "    return resume_texts, resume_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7810efab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(diro + \"/data/skill_patterns.jsonl\") as f:\n",
    "    created_entities = [line['label'].upper() for line in f.iter()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ad5cb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_newruler_to_pipeline(skill_pattern_path):\n",
    "    '''Reads in all created patterns from a JSONL file and adds it to the pipeline after PARSER and before NER'''\n",
    "    \n",
    "    new_ruler = EntityRuler(nlp).from_disk(skill_pattern_path)\n",
    "    nlp.add_pipe(new_ruler, after='parser')\n",
    "    \n",
    "\n",
    "def visualize_entity_ruler(entity_list, doc):\n",
    "    '''Visualize the Skill entities of a doc'''\n",
    "    \n",
    "    options = {\"ents\": entity_list}\n",
    "    displacy.render(doc, style='ent', options=options)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1dbc4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e43010cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Beginning - manual input:</br>Dear Ms. Mustermann, with this letter and attached resume, I would like to express my sincere interest in the vacancy for ‘Data Specialist’.&quot;</br>===============================================================================================================================================================</br></br>As a highly skilled and highly accomplished professional with comprehensive expertise in performing all facets of data entry and data management, I am confident that I would significantly contribute to the success of your organization.</br></br>My career accomplishments include more than four years of experience in positions of increasing responsibility within the Data Entry and Analysis Center at Juniper Systems. In addition to this, my comprehensive knowledge of quality assurance and storage techniques is certain to render me an asset to your organization. Furthermore, my superior communication and problem-solving skills position me to make a significant and positive impact on your organization.</br></br>The following achievements highlight my qualifications for this position:</br></br> Increased data entry and data management efficiency by 32% and 46% across all blue team sites across multiple assignments across multiple data centers across the Midwest region; led teams through the creation of detailed descriptions of each sample item, along with the corresponding sample conditionals, and conditionals followed by the corresponding results.</br></br> Led teams through the creation of detailed descriptions of each sample item, along with corresponding results.</br></br> Led through and executed a drastic reduction in data entry and data loss by 36%, 41%, and 19%, respectively, across three data centers across the Midwest region.</br></br> Led teams through the creation of detailed descriptions of each sample item, along with a sample condition—’to facilitate the proper handling and presentation of data by Juniper Systems.</br></br> Led through major enhancements to the Juniper system by analyzing the data and introducing a new conditionals procedure to enhance the efficiency of the Juniper sample analysis process.</br></br> Improved database performance by 46% by introducing a new operations database to optimize performance of all the data entry databases in Juniper.</br></br> Implemented a data migration from a current 90KB monthly cycle to a new 32KB monthly cycle, maximizing efficiency at reducing incoming transaction costs.</br></br> Maintained a significantly improved database with over 60,000 registered users across all databases.</br></br> Achieved an overall completion rate of 87.9%, including all testing.</br></br>My proven dedication to facilitating efficient and accurate data entry and analysis for Juniper Systems, along with my exceptional analytical and problem-solving talents, will contribute immensely to the success of your team. Thank you for your consideration, and I look forward to speaking with you soon.</br></br></br>====================</br> As an experienced and motivated professional with more than seven years of experience driving high-performance, data-driven operations, I possess a range of knowledge and skills that will allow me to contribute toward the success of your organization.</br></br>My expertise lies in successfully creating, maintaining, and rolling out new systems and processes, as well as overseeing testing and troubleshooting operations to ensure all systems and processes meet corporate goals. Through my experience, I have become well versed in overseeing a wide variety of data management and analysis tasks, while simultaneously ensuring top-level accuracy and attention to detail. My additional success in building key relationships through key partnerships positions me to make a significant contribution to your organization.</br></br>The following achievements demonstrate my qualification for this position:</br></br> Spearheading data collection, presentation, and data processing operations for multiple organizations, working collaboratively with a highly accomplished executive team at JPI Systems, culminating in the role of Data Specialist.</br></br> Developing strategic initiatives and plans to propel sales performance while managing vendor negotiations and leading high-performance teams to achieve maximum productivity and efficiency.</br></br> Analyzing and managing comprehensive documentation, records, and reportkeeping systems while optimizing operational strategies and mitigating risk by analyzing operations through dynamic analytical thinking and documentation.</br></br> Managing a range of project management and client relationship responsibilities, including scheduling, calendars, mock presentations, RFPs, and issue resolution.</br></br> Producing detailed, comprehensive documentation, including correspondence and project management, to all parties to ensure implementation and execution.</br></br> Demonstrating superior communication and relationship management talents.</br></br>My proven success in meeting corporate data goals, along with my comprehensive expertise in driving data quality and progress, will contribute immensely to the success of your organization. Thank you for your consideration; I look forward to speaking with you soon.</br></br></br>====================</br> As an analytical and top-performing professional with a strong history of supporting and exceeding data integrity and quality, I possess the skills and qualifications to enable me to contribute toward the success of your organization.</br></br>My experience includes successfully analyzing customer data sets to drive data quality and consistency, and subsequently leading support teams to respond to and resolve data problems. My established success in advising senior executives on data breaches and other critical issues—along with my expertise in comprehensive data analysis tools and systems—positions me to make a significant and positive impact on your organization.</br></br>Consider the following highlights of my achievements:</br></br> Excelled as a Data Specialist with Next Level Manufacturing for the past six years, supporting data entry and quality control with the production of more than 550,000 units.</br></br> Led all aspects of quality assurance—including audits and work releases—for all production lines during my tenure with Next Level Manufacturing.</br></br> Provided essential team leadership, such as keeping line engineers motivated and running tests and copy edits to ensure goal achievement.</br></br> Led all phases of quality control studies; preparing reports, maintaining Excel spreadsheets, and developing specifications for each production line to ensure proper functioning.</br></br> Played a key role in improving process efficiency by assigning tasks to more employees and reducing downtime.</br></br> Earned a Bachelor’s degree in Engineering from Duke University.</br></br>My proven dedication to optimizing product integration and customer support, along with my excellent analytical and problem-solving talents, will contribute immensely to the success of your company. Thank you for your consideration, and I look forward to speaking with you soon.</br></br></br>====================</br> As a highly skilled and accomplished professional with more than 11 years of experience assisting and strategizing with data collection and data management, I am confident that I would significantly contribute to the success of your team.</br></br>My experience lies in successfully facilitating the data collection, analysis, and analysis of various medical articles from various sources, as well as in providing overarching administrative and operational support to senior management teams. Throughout my career, I have demonstrated an unparalleled dedication to providing outstanding support within fast-paced, customer-facing environments. Additionally, my demonstrated success in overseeing administrative and operational strategies within healthcare industry organizations, positions me to make a significant impact on your organization.</br></br>The following achievements demonstrate my qualification for this position:</br></br> Propelling research productivity and quantile growth, improving efficiency by 28%, reducing expenses by 24%, and improving accuracy by 31%.</br></br> Facilitating the participation of medical teams and marketing teams to identify and recommended treatment plans to meet patient needs and expectations.</br></br> Communicating effectively with physicians, nurses, medical assistants, and senior management staff to facilitate highly successful healthcare recruitment and management.</br></br> Earning an MBA in Health Information Management from Western University; holding dual Master’s degrees in Medical Information and Management from the University of Pennsylvania.</br></br>My proven dedication to optimizing organizational success through my expert knowledge of medical research and management will contribute immensely to the success of your team. Thank you for your consideration, and I look forward to speaking with you soon.</br></br></br>====================</br> As a highly skilled and accomplished professional with extensive experience overseeing comprehensive data collection and manipulation, I possess a wide range of knowledge and abilities that will allow me to contribute toward the success of your organization.</br></br>My expertise lies in successfully analyzing complex data sets to identify and document weaknesses, gaps, and discrepancies. I excel at liaising with customers and implementing highly effective data management systems while continually completing appropriate training programs to advance skillful data analysis. Furthermore, my established success in supervising and motivating teams to top performance levels positions me to make a significant contribution to your organization.</br></br>The following achievements demonstrate my qualification for this position:</br></br> Overseeing comprehensive data collection and management tasks within high-pressure environments while identifying and swiftly resolving errors and/or discrepancies.</br></br> Developing and maintaining standard-setting compliance documents and tickets, maintaining conditioner stations and inspecting samples to maintain strict compliance.</br></br> Creating and maintaining a variety of presentations, reports, KPIs, and RFP items, including requirement drawings and descriptions.</br></br> Performing daily system audits and identifying performance gaps by analyzing methods and procedures implemented methodologies previously unfortunately attributed to management.</br></br> Preparing to achieve a Master’s degree in Business Information Science from the University of California, San Diego.</br></br>My proven dedication to optimizing customer data service success through my expert knowledge of dynamic data management techniques will contribute immensely to the success of your team. Thank you for your consideration, and I look forward to speaking with you soon.</br></br></br>====================</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#open text file in read mode\n",
    "text_file = open(\"Data Specialist-500 tokens.txt\", \"r\")\n",
    " \n",
    "#read whole file to a string\n",
    "data = text_file.read()\n",
    " \n",
    "#close file\n",
    "text_file.close()\n",
    "doc = nlp(data)\n",
    "visualize_entity_ruler(created_entities, doc)\n",
    "# for ent in doc.ents:\n",
    "#     print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5c8c029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_skillset_dict(resume_names, resume_texts):\n",
    "    '''Create a dictionary containing a set of the extracted skills. Name is key, matching skillset is value'''\n",
    "    skillsets = [create_skill_set(resume_text) for resume_text in resume_texts]\n",
    "\n",
    "    return dict(zip(resume_names, skillsets))\n",
    "\n",
    "def match_skills(vacature_set, cv_set, resume_name):\n",
    "    '''Get intersection of resume skills and job offer skills and return match percentage'''\n",
    "    \n",
    "    if len(vacature_set) < 1:\n",
    "        print('could not extract skills from job offer text')   \n",
    "    else:\n",
    "        pct_match = round(len(vacature_set.intersection(cv_set[resume_name])) / len(vacature_set) * 100, 0)\n",
    "        print(resume_name + \" has a {}% skill match on this job offer\".format(pct_match))\n",
    "        print('Required skills: {} '.format(vacature_set))\n",
    "        print('Matched skills: {} \\n'.format(vacature_set.intersection(skillset_dict[resume_name])))\n",
    "        \n",
    "        return (resume_name, pct_match)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f151ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skills(text):\n",
    "    doc = nlp(text)\n",
    "    myset = []\n",
    "    subset = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_==\"SKILL\":\n",
    "             subset.append(ent.text)\n",
    "    myset.append(subset)\n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "704a3894",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_skill_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_88445/2960571796.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresume_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_tokenized_texts_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mskillset_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_skillset_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# example of job offer text (string). Can input your own.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_88445/3575722265.py\u001b[0m in \u001b[0;36mcreate_skillset_dict\u001b[0;34m(resume_names, resume_texts)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_skillset_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m'''Create a dictionary containing a set of the extracted skills. Name is key, matching skillset is value'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mskillsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcreate_skill_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_text\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresume_text\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresume_texts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskillsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_88445/3575722265.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_skillset_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m'''Create a dictionary containing a set of the extracted skills. Name is key, matching skillset is value'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mskillsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcreate_skill_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_text\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresume_text\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresume_texts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskillsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_skill_set' is not defined"
     ]
    }
   ],
   "source": [
    "#already done with the instantiation of created_entites, here commented for future reference\n",
    "#add_newruler_to_pipeline(diro+\"/data/skill_patterns.jsonl\")\n",
    "\n",
    "resume_texts, resume_names = create_tokenized_texts_list(extension)\n",
    "\n",
    "skillset_dict = create_skillset_dict(resume_names, resume_texts)\n",
    "\n",
    "# example of job offer text (string). Can input your own.\n",
    "vacature_text = vacatures_df[vacatures_df['soort_vacature'] == 'Data Scientist'].skills.iloc[13]\n",
    "\n",
    "# Create a set of the skills extracted from the job offer text\n",
    "vacature_skillset = create_skill_set(nlp(vacature_text))\n",
    "\n",
    "# Create a list with tuple pairs containing the names of the candidates and their match percentage\n",
    "match_pairs = [match_skills(vacature_skillset, skillset_dict, name) for name in skillset_dict.keys()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "00f026f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'match_pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_68692/1693509320.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Sort tuples from high to low on match percentage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmatch_pairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Unpack tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmatch_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'match_pairs' is not defined"
     ]
    }
   ],
   "source": [
    "# Sort tuples from high to low on match percentage\n",
    "match_pairs.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "# Unpack tuples\n",
    "names, pct = zip(*match_pairs)\n",
    "\n",
    "# Plotting\n",
    "sns.set_theme(style='darkgrid')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.set_title('Job offer match with candidates', fontsize=20)\n",
    "ax.set(xlabel='Candidates', ylabel='% Match')\n",
    "ax.set(ylim=(0, 100))\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "sns.barplot(x=list(names), y=list(pct), color='b')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76fc0add",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-07 19:55:56.804016: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-09-07 19:55:56.804075: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (absconditus-82b1): /proc/driver/nvidia/version does not exist\n",
      "2022-09-07 19:55:56.805133: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, GRU\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "word_dim = 50\n",
    "num_tokens = 15000\n",
    "\n",
    "# Define the layers\n",
    "word_vec_input = Input(shape=(word_dim,))\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embed = Embedding(input_dim=num_tokens, output_dim=word_dim, mask_zero=True)\n",
    "decoder_gru_1 = GRU(word_dim, return_sequences=True, return_state=False)\n",
    "decoder_gru_2 = GRU(word_dim, return_sequences=True, return_state=True)\n",
    "decoder_dense = Dense(num_tokens, activation='softmax')\n",
    "\n",
    "# Connect the layers\n",
    "embedded = decoder_embed(decoder_inputs)\n",
    "gru_1_output = decoder_gru_1(embedded, initial_state=word_vec_input)\n",
    "gru_2_output, state_h = decoder_gru_2(gru_1_output)\n",
    "decoder_outputs = decoder_dense(gru_2_output)\n",
    "\n",
    "# Define the model that will be used for training\n",
    "training_model = Model([word_vec_input, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Also create a model for inference (this returns the GRU state)\n",
    "decoder_model = Model([word_vec_input, decoder_inputs], [decoder_outputs, state_h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20283ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll need these libraries to gather and shape the data.\n",
    "import requests \n",
    "import pandas as pd\n",
    "from itertools import compress\n",
    "\n",
    "# This will return the data for all the cards available to scryfall.\n",
    "r = requests.get('https://c2.scryfall.com/file/scryfall-bulk/all-cards/all-cards-20200831091816.json')\n",
    "data = r.json()\n",
    "\n",
    "# we'll start parsing by removing any cards with no flavor text\n",
    "contains_x = []\n",
    "for i in data:\n",
    "    contains_x.append('flavor_text' in i.keys())\n",
    "\n",
    "data_filtered = list(compress(data, contains_x))\n",
    "\n",
    "# next we'll remove any cards in a language other than english\n",
    "contains_y = []\n",
    "for i in data_filtered:\n",
    "  contains_y.append('lang' in i.keys() and 'en' == i['lang'])\n",
    "\n",
    "data_filtered = list(compress(data_filtered, contains_y))\n",
    "\n",
    "# Now we'll create a list to iterate through.\n",
    "cardValues = []\n",
    "for i in data_filtered:\n",
    "    cardValues.append(i['flavor_text'])\n",
    "\n",
    "# I'll convert this to a data frame to visualize a few rows nicely\n",
    "# mostly just a sanity check.\n",
    "df = pd.DataFrame(\n",
    "    cardValues,\n",
    "    columns=['data']\n",
    "    )\n",
    "\n",
    "cards = df.data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6668b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"A rift opened, and our arrows were abruptly s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"We take only what we need to survive. Believe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"May this blade guide you on your great journe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A howl on the wind hides many dangers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The problem wasn't that fish had learned how t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data\n",
       "0  \"A rift opened, and our arrows were abruptly s...\n",
       "1  \"We take only what we need to survive. Believe...\n",
       "2  \"May this blade guide you on your great journe...\n",
       "3             A howl on the wind hides many dangers.\n",
       "4  The problem wasn't that fish had learned how t..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9577131",
   "metadata": {},
   "outputs": [],
   "source": [
    "skillz = \"/home/absconditus/Documents/Github/Python Projects/jupyter/TF LSTM RNN for cover letter generation/Workfiles/data/skill_patterns.jsonl\"\n",
    "with jsonlines.open(skillz) as f:\n",
    "    created_entities = [line['label'].upper() for line in f.iter()]\n",
    "\n",
    "ruler = EntityRuler(nlp).from_disk(skillz)\n",
    "nlp.add_pipe(ruler, after='parser')\n",
    "\n",
    "def get_skills(text):\n",
    "    doc = nlp(text)\n",
    "    myset = []\n",
    "    subset = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_==\"SKILL\":\n",
    "             subset.append(ent.text)\n",
    "    myset.append(subset)\n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba14cea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open text file in read mode\n",
    "text_file = open(\"Data Specialist-500 tokens.txt\", \"r\")\n",
    " \n",
    "#read whole file to a string\n",
    "data = text_file.read()\n",
    " \n",
    "#close file\n",
    "text_file.close()\n",
    "\n",
    "pb = pd.DataFrame(\n",
    "    columns=['title','body']\n",
    "    )\n",
    "pb.loc[0]='Data Specialist',data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2031b3c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Specialist</td>\n",
       "      <td>Beginning - manual input:\\nDear Ms. Mustermann...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             title                                               body\n",
       "0  Data Specialist  Beginning - manual input:\\nDear Ms. Mustermann..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "90200bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pb['skills']=pb['body'].apply(get_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f689669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Specialist</td>\n",
       "      <td>Beginning - manual input:\\nDear Ms. Mustermann...</td>\n",
       "      <td>[data management, data management, database, d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             title                                               body  \\\n",
       "0  Data Specialist  Beginning - manual input:\\nDear Ms. Mustermann...   \n",
       "\n",
       "                                              skills  \n",
       "0  [data management, data management, database, d...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3e13f14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.12.1-cp39-cp39-manylinux1_x86_64.whl (776.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.4/776.4 MB\u001b[0m \u001b[31m760.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/absconditus/anaconda3/lib/python3.9/site-packages (from torch) (3.10.0.2)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.12.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bac53b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.12.1+cu102\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import zipfile\n",
    "import random\n",
    "import time\n",
    "import csv\n",
    "import datetime\n",
    "from itertools import compress\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForPreTraining, \\\n",
    "                         AdamW, get_linear_schedule_with_warmup, \\\n",
    "                         TrainingArguments, BeamScorer, Trainer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split, DataLoader, \\\n",
    "                             RandomSampler, SequentialSampler\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c6355eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG           = False\n",
    "\n",
    "INPUT_DIR       = 'articles'\n",
    "\n",
    "USE_APEX        = True\n",
    "APEX_OPT_LEVEL  = 'O1'\n",
    "\n",
    "MODEL           = 'gpt2' #{gpt2, gpt2-medium, gpt2-large, gpt2-xl}\n",
    "\n",
    "UNFREEZE_LAST_N = 6 #The last N layers to unfreeze for training\n",
    "\n",
    "SPECIAL_TOKENS  = { \"bos_token\": \"<|BOS|>\",\n",
    "                    \"eos_token\": \"<|EOS|>\",\n",
    "                    \"unk_token\": \"<|UNK|>\",                    \n",
    "                    \"pad_token\": \"<|PAD|>\",\n",
    "                    \"sep_token\": \"<|SEP|>\"}\n",
    "                    \n",
    "MAXLEN          = 768  #{768, 1024, 1280, 1600}\n",
    "\n",
    "TRAIN_SIZE      = 0.8\n",
    "\n",
    "if USE_APEX:\n",
    "    TRAIN_BATCHSIZE = 4\n",
    "    BATCH_UPDATE    = 16\n",
    "else:\n",
    "    TRAIN_BATCHSIZE = 2\n",
    "    BATCH_UPDATE    = 32\n",
    "\n",
    "EPOCHS          = 4\n",
    "LR              = 5e-4\n",
    "EPS             = 1e-8\n",
    "WARMUP_STEPS    = 1e2\n",
    "\n",
    "SEED            = 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "463cf16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "302a8a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip'\n",
    "r = requests.get(url)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "483ccec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df size: 422,419\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>URL</th>\n",
       "      <th>PUBLISHER</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>Alphanumeric ID</th>\n",
       "      <th>HOSTNAME Url</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Fed official says weak data caused by weather,...</td>\n",
       "      <td>http://www.latimes.com/business/money/la-fi-mo...</td>\n",
       "      <td>Los Angeles Times</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.latimes.com</td>\n",
       "      <td>1394470370698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Fed's Charles Plosser sees high bar for change...</td>\n",
       "      <td>http://www.livemint.com/Politics/H2EvwJSK2VE6O...</td>\n",
       "      <td>Livemint</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.livemint.com</td>\n",
       "      <td>1394470371207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                              TITLE  \\\n",
       "0   1  Fed official says weak data caused by weather,...   \n",
       "1   2  Fed's Charles Plosser sees high bar for change...   \n",
       "\n",
       "                                                 URL          PUBLISHER  \\\n",
       "0  http://www.latimes.com/business/money/la-fi-mo...  Los Angeles Times   \n",
       "1  http://www.livemint.com/Politics/H2EvwJSK2VE6O...           Livemint   \n",
       "\n",
       "  CATEGORY                Alphanumeric ID      HOSTNAME Url      TIMESTAMP  \n",
       "0        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM   www.latimes.com  1394470370698  \n",
       "1        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM  www.livemint.com  1394470371207  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['ID',\n",
    "           'TITLE',\n",
    "           'URL',\n",
    "           'PUBLISHER',\n",
    "           'CATEGORY', #News category (b = business, t = science and technology, e = entertainment, m = health)\n",
    "           'Alphanumeric ID',\n",
    "           'HOSTNAME Url',\n",
    "           'TIMESTAMP']\n",
    "\n",
    "df = pd.read_csv(\"newsCorpora.csv\", sep='\\t', header=None, names=columns)\n",
    "print(f\"df size: {len(df) :,}\")\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6320062",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, tokenizer, randomize=True):\n",
    "\n",
    "        title, text, keywords = [], [], []\n",
    "        for k, v in data.items():\n",
    "            title.append(v[0])\n",
    "            text.append(v[1])\n",
    "            keywords.append(v[2])\n",
    "\n",
    "        self.randomize = randomize\n",
    "        self.tokenizer = tokenizer \n",
    "        self.title     = title\n",
    "        self.text      = text\n",
    "        self.keywords  = keywords  \n",
    "\n",
    "    #---------------------------------------------#\n",
    "\n",
    "    @staticmethod\n",
    "    def join_keywords(keywords, randomize=True):\n",
    "        N = len(keywords)\n",
    "\n",
    "        #random sampling and shuffle\n",
    "        if randomize: \n",
    "            M = random.choice(range(N+1))\n",
    "            keywords = keywords[:M]\n",
    "            random.shuffle(keywords)\n",
    "\n",
    "        return ','.join(keywords)\n",
    "\n",
    "    #---------------------------------------------#\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    #---------------------------------------------#\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        keywords = self.keywords[i].copy()\n",
    "        kw = self.join_keywords(keywords, self.randomize)\n",
    "        \n",
    "        input = SPECIAL_TOKENS['bos_token'] + self.title[i] + \\\n",
    "                SPECIAL_TOKENS['sep_token'] + kw + SPECIAL_TOKENS['sep_token'] + \\\n",
    "                self.text[i] + SPECIAL_TOKENS['eos_token']\n",
    "\n",
    "        encodings_dict = tokenizer(input,                                   \n",
    "                                   truncation=True, \n",
    "                                   max_length=MAXLEN, \n",
    "                                   padding=\"max_length\")   \n",
    "        \n",
    "        input_ids = encodings_dict['input_ids']\n",
    "        attention_mask = encodings_dict['attention_mask']\n",
    "        \n",
    "        return {'label': torch.tensor(input_ids),\n",
    "                'input_ids': torch.tensor(input_ids), \n",
    "                'attention_mask': torch.tensor(attention_mask)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a6fb55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, S=TRAIN_SIZE):\n",
    "    # Shuffle ids\n",
    "    ids = list(data.keys())\n",
    "    random.shuffle(ids)\n",
    "\n",
    "    # Split into training and validation sets    \n",
    "    train_size = int(S * len(data))\n",
    "\n",
    "    train_ids = ids[:train_size]\n",
    "    val_ids = ids[train_size:]\n",
    "\n",
    "    train_data = dict()\n",
    "    for id in train_ids:\n",
    "        train_data[id] = data[id]\n",
    "\n",
    "    val_data = dict()\n",
    "    for id in val_ids:\n",
    "        val_data[id] = data[id]\n",
    "\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1b48ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenier(special_tokens=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL) #GPT2Tokenizer\n",
    "\n",
    "    if special_tokens:\n",
    "        tokenizer.add_special_tokens(special_tokens)\n",
    "        print(\"Special tokens added\")\n",
    "    return tokenizer\n",
    "\n",
    "def get_model(tokenizer, special_tokens=None, load_model_path=None):\n",
    "\n",
    "    #GPT2LMHeadModel\n",
    "    if special_tokens:\n",
    "        config = AutoConfig.from_pretrained(MODEL, \n",
    "                                            bos_token_id=tokenizer.bos_token_id,\n",
    "                                            eos_token_id=tokenizer.eos_token_id,\n",
    "                                            sep_token_id=tokenizer.sep_token_id,\n",
    "                                            pad_token_id=tokenizer.pad_token_id,\n",
    "                                            output_hidden_states=False)\n",
    "    else: \n",
    "        config = AutoConfig.from_pretrained(MODEL,                                     \n",
    "                                            pad_token_id=tokenizer.eos_token_id,\n",
    "                                            output_hidden_states=False)    \n",
    "\n",
    "    #----------------------------------------------------------------#\n",
    "    model = AutoModelForPreTraining.from_pretrained(MODEL, config=config)\n",
    "\n",
    "    if special_tokens:\n",
    "        #Special tokens added, model needs to be resized accordingly\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if load_model_path:\n",
    "        model.load_state_dict(torch.load(load_model_path))\n",
    "\n",
    "#     model.cuda()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "718bd7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens added\n",
      "CPU times: user 6.19 s, sys: 482 ms, total: 6.67 s\n",
      "Wall time: 3.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = get_tokenier(special_tokens=SPECIAL_TOKENS)\n",
    "model = get_model(tokenizer, \n",
    "                  special_tokens=SPECIAL_TOKENS,\n",
    "                #   load_model_path='pytorch_model.bin'\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "490790c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Freeze selective layers:\n",
    "# - Freeze all layers except last n:\n",
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "for i, m in enumerate(model.transformer.h):        \n",
    "    #Only un-freeze the last n transformer blocks\n",
    "    if i+1 > 12 - UNFREEZE_LAST_N:\n",
    "        for parameter in m.parameters():\n",
    "            parameter.requires_grad = True \n",
    "\n",
    "for parameter in model.transformer.ln_f.parameters():        \n",
    "    parameter.requires_grad = True\n",
    "\n",
    "for parameter in model.lm_head.parameters():        \n",
    "    parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e325eae1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_46823/3453630071.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "train_data, val_data = split_data(data)\n",
    "\n",
    "train_dataset = myDataset(train_data, tokenizer)\n",
    "val_dataset = myDataset(val_data, tokenizer, randomize=False)\n",
    "\n",
    "f'There are {len(train_dataset) :,} samples for training, and {len(val_dataset) :,} samples for validation testing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0612e94f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "--load_best_model_at_end requires the save and eval strategy to match, but found\n- Evaluation strategy: epoch\n- Save strategy: steps",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_on_each_node, no_cuda, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, xpu_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_best_model_at_end\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_strategy\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_strategy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   1025\u001b[0m                     \u001b[0;34m\"--load_best_model_at_end requires the save and eval strategy to match, but found\\n- Evaluation \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m                     \u001b[0;34mf\"strategy: {self.evaluation_strategy}\\n- Save strategy: {self.save_strategy}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: --load_best_model_at_end requires the save and eval strategy to match, but found\n- Evaluation strategy: epoch\n- Save strategy: steps"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BATCHSIZE,\n",
    "    per_device_eval_batch_size=TRAIN_BATCHSIZE,\n",
    "    gradient_accumulation_steps=BATCH_UPDATE,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    fp16_opt_level=APEX_OPT_LEVEL,\n",
    "    warmup_steps=WARMUP_STEPS,    \n",
    "    learning_rate=LR,\n",
    "    adam_epsilon=EPS,\n",
    "    weight_decay=0.01,        \n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,     \n",
    ")\n",
    "\n",
    "#---------------------------------------------------#\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,    \n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "#---------------------------------------------------#\n",
    "trainer.train()\n",
    "trainer.save_model()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "304ac593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens added\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pytorch_model.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_46823/3147248091.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspecial_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSPECIAL_TOKENS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model = get_model(tokenizer, \n\u001b[0m\u001b[1;32m      3\u001b[0m                   \u001b[0mspecial_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSPECIAL_TOKENS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                   load_model_path='pytorch_model.bin')\n",
      "\u001b[0;32m/tmp/ipykernel_46823/2338185166.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(tokenizer, special_tokens, load_model_path)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mload_model_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pytorch_model.bin'"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenier(special_tokens=SPECIAL_TOKENS)\n",
    "model = get_model(tokenizer, \n",
    "                  special_tokens=SPECIAL_TOKENS,\n",
    "                  load_model_path='pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e84c5dc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_46823/2331963406.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "title = \"We got a lot of grief when our photo became a meme\"\n",
    "keywords = ['train', 'lads', 'drinking', 'picture', 'funny', 'instagram']\n",
    "kw = myDataset.join_keywords(keywords, randomize=False)\n",
    "\n",
    "prompt = SPECIAL_TOKENS['bos_token'] + title + \\\n",
    "         SPECIAL_TOKENS['sep_token'] + kw + SPECIAL_TOKENS['sep_token']\n",
    "         \n",
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "device = torch.device(\"cuda\")\n",
    "generated = generated.to(device)\n",
    "\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "374f54d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_46823/1321029287.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Top-p (nucleus) text generation (10 samples):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m sample_outputs = model.generate(generated, \n\u001b[0m\u001b[1;32m      3\u001b[0m                                 \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                 \u001b[0mmin_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                 \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAXLEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Top-p (nucleus) text generation (10 samples):\n",
    "sample_outputs = model.generate(generated, \n",
    "                                do_sample=True,   \n",
    "                                min_length=50, \n",
    "                                max_length=MAXLEN,\n",
    "                                top_k=30,                                 \n",
    "                                top_p=0.7,        \n",
    "                                temperature=0.9,\n",
    "                                repetition_penalty=2.0,\n",
    "                                num_return_sequences=10\n",
    "                                )\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    text = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "    a = len(title) + len(','.join(keywords))    \n",
    "    print(\"{}: {}\\n\\n\".format(i+1,  text[a:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e044f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam-search text generation:\n",
    "sample_outputs = model.generate(generated, \n",
    "                                do_sample=True,   \n",
    "                                max_length=MAXLEN,                                                      \n",
    "                                num_beams=5,\n",
    "                                repetition_penalty=5.0,\n",
    "                                early_stopping=True,      \n",
    "                                num_return_sequences=1\n",
    "                                )\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    text = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "    a = len(title) + len(','.join(keywords))    \n",
    "    print(\"{}: {}\\n\\n\".format(i+1,  text[a:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caefd5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenier()\n",
    "model = get_model(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45eee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = title\n",
    "\n",
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "device = torch.device(\"cuda\")\n",
    "generated = generated.to(device)\n",
    "\n",
    "model.eval()\n",
    "sample_outputs = model.generate(generated, \n",
    "                                do_sample=True,   \n",
    "                                max_length=MAXLEN,                                                      \n",
    "                                num_beams=5,\n",
    "                                repetition_penalty=5.0,\n",
    "                                early_stopping=True,      \n",
    "                                num_return_sequences=1\n",
    "                                )\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Resuyay",
   "language": "python",
   "name": "resuyay"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
